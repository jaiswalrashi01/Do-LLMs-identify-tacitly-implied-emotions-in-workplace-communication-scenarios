# Do-LLMs-identify-tacitly-implied-emotions-in-workplace-communication-scenarios
**Investigated how LLMs (GPT-4o, Phi-3) detect tacit emotions in 250 workplace scenarios. Using free-form vs. list-based prompting and human comparison, results show very low human agreement but modest AI-AI alignment, highlighting challenges of masked affect, rare nuanced emotions to more emotionally intelligent AI.**

This study explores how effectively large language models (LLMs) detect tacit emotions in workplace communication, where affect is rarely expressed explicitly but implied through tone, structure, or professional framing. Emails, chat updates, and meeting notes often adopt a neutral or formal register that masks underlying states such as frustration, disappointment, or resignation. Existing sentiment analysis tools largely capture overt polarity, leaving tacitly implied emotions underexplored. Given the rapid adoption of generative AI in business contexts, assessing whether LLMs can reliably recognize these signals is critical for organizational health, governance, and decision-making.

To address this gap, the study introduces a novel dataset of 250 workplace prompts covering marketing, HR, and product development contexts. These prompts were crafted to embed emotional ambiguity in realistic scenarios, with demographic variation and professional tone. Two annotators independently labelled emotions using Plutchik’s model as a loose reference, permitting multi-label interpretations while preserving disagreement to reflect subjectivity. This provided a foundation for comparing human-human, human-model, and model-model consistency.

Two prompting approaches were tested: free-form, allowing LLMs to infer emotions without constraints, and selected-list, forcing them to choose from a hidden emotion set. Neutral reset prompts were inserted between tasks to reduce priming effects. Responses were generated programmatically using GPT-4o and Phi-3 Mini 4K Instruct. For benchmarking, a GoEmotions transformer was also applied with lexicon mapping and symbolic augmentation.

Quantitative evaluation used Jaccard Similarity, Cohen’s Kappa, and Micro/Macro F1 scores. Results revealed extremely low agreement between human annotators (Jaccard 0.037), underscoring the difficulty of labeling tacit emotions. LLMs showed higher consistency with each other (Jaccard 0.13) than with humans, but both free-form and selected-list approaches had limitations. Free-form elicitation produced richer but variable outputs, while constrained lists reduced diversity and distorted nuance.

Overall, the study highlights the challenges of emotion annotation in professional contexts, where subjectivity, tone, and implicit framing complicate reliable detection. It underscores both the promise and current limits of LLMs as emotionally intelligent tools and suggests future directions, including multi-annotator calibration, refined emotion ontologies, and fine-tuning with soft labels to bridge the gap between distributional AI inference and embodied human emotional reasoning.
